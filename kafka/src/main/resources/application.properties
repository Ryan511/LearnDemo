#============== kafka ===================
# 指定kafka 代理地址，可以多个
#spring.kafka.bootstrap-servers=127.0.0.1:9091,127.0.0.1:9092,127.0.0.1:9093
#=============== provider  =======================
#spring.kafka.producer.retries=0
# 每次批量发送消息的数量
#spring.kafka.producer.batch-size=16384
#spring.kafka.producer.buffer-memory=33554432
# 指定消息key和消息体的编解码方式
#spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
#spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
#=============== consumer  =======================
# 指定默认消费者group id
#spring.kafka.consumer.group-id=user-log-group
#spring.kafka.consumer.auto-offset-reset=earliest
#spring.kafka.consumer.enable-auto-commit=true
#spring.kafka.consumer.auto-commit-interval=100
# 指定消息key和消息体的编解码方式
#spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
#spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
#==================kafka生产者
kafka_topic=test-3
#brokers集群
kafka_bootstrap_servers=127.0.0.1:9091,127.0.0.1:9092,127.0.0.1:9093
#kafka_bootstrap_servers=192.168.199.236:19092
#即所有副本都同步到数据时send方法才返回, 以此来完全判断数据是否发送成功, 理论上来讲数据不会丢失.
kafka_acks=all
#发送失败重试次数
kafka_retries=65535
#批处理条数：当多个记录被发送到同一个分区时，生产者会尝试将记录合并到更少的请求中。这有助于客户端和服务器的性能。
kafka_batch_size=16384
#批处理延迟时间上限：即1ms过后，不管是否达到批处理数，都直接发送一次请求
kafka_linger_ms=1
kafka_key_serializer=org.apache.kafka.common.serialization.StringSerializer
kafka_value_serializer=org.apache.kafka.common.serialization.StringSerializer
#即32MB的批处理缓冲区
kafka_buffer_memory=33554432
#========kafka消费者
#消费者群组ID，发布-订阅模式，即如果一个生产者，多个消费者都要消费，那么需要定义自己的群组，同一群组内的消费者只有一个能消费到消息
kafka_group_id=order-beta
#如果为true，消费者的偏移量将在后台定期提交。
enable_auto_commit=false
kafka_key_deserializer=org.apache.kafka.common.serialization.StringDeserializer
kafaka_value_deserializer=org.apache.kafka.common.serialization.StringDeserializer
#如何设置为自动提交（enable.auto.commit=true），这里设置自动提交周期
auto.commit.interval.ms=1000
#在使用Kafka的组管理时，用于检测消费者故障的超时
session.timeout.ms=15000
#消费监听器容器并发数
concurrency=3
#KAFKA数据链接参数
consumer_group_id=group1
#logging.level.root=debug